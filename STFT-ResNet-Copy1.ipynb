{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e407538f-9791-4b80-936f-05bf644f1689",
   "metadata": {},
   "source": [
    "# STFT-ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95874883-2b9b-4d28-a6fc-66f3915a330b",
   "metadata": {},
   "source": [
    "## 1. Import & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07c7e2-dfbb-4ddb-a938-9651dff727b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.data_loader import csv_to_npy, load_trace, load_supervised_set, AES_VERSIONS, IS_RAND, RAW_DATASET_PATH, DATASET_PATH\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from scipy.signal import stft\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc1cea-9f98-403f-b95e-66006a49e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a1de0-2413-407f-82e5-716f6100e58e",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b9443-d3c1-4c19-b542-51c07f9fac83",
   "metadata": {},
   "source": [
    "### 2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16720a-8d66-4c14-bcb5-6f43890a60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet(model_name='resnet18', num_classes=1):\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = models.resnet34(weights=None)\n",
    "\n",
    "    original_conv1 = model.conv1\n",
    "    # 3 channel -> 1 channel\n",
    "    model.conv1 = nn.Conv2d(1, original_conv1.out_channels, kernel_size=original_conv1.kernel_size, \n",
    "                            stride=original_conv1.stride, padding=original_conv1.padding, bias=False)\n",
    "\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e79181-4cbc-4bbb-91c3-a0eb083776be",
   "metadata": {},
   "source": [
    "### 2.2 STFT-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e532be-c8bd-41f5-8b42-5d22b71db6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTDataset(Dataset):\n",
    "    def __init__(self, X, y, fit_on_data=None, fs=2e9, nperseg=128, noverlap=96):\n",
    "        self.y = torch.tensor(y.astype(np.float32)).view(-1, 1)\n",
    "        self.spectrograms = []\n",
    "\n",
    "        print(\"Generating spectrograms\")\n",
    "        db_spectrograms = []\n",
    "        for i in tqdm(range(len(X))):\n",
    "            waveform = X[i]\n",
    "            _, _, Zxx = stft(waveform, fs=fs, nperseg=nperseg, noverlap=noverlap, boundary=None, padded=False)\n",
    "            Sxx = np.abs(Zxx)\n",
    "            Sxx_db = 20 * np.log10(Sxx + 1e-9)\n",
    "            db_spectrograms.append(Sxx_db)\n",
    "\n",
    "        self.target_shape = db_spectrograms[0].shape\n",
    "        print(f\"STFT shape: {self.target_shape}\")\n",
    "            \n",
    "        if fit_on_data is not None: # global min-max scaling\n",
    "            self.min_val = fit_on_data.min_val\n",
    "            self.max_val = fit_on_data.max_val\n",
    "        else:\n",
    "            # global min-max scaling on Train Set\n",
    "            print(\"Calculating global min/max normalization\")\n",
    "            all_sxx_db = np.stack(db_spectrograms)\n",
    "            self.min_val = all_sxx_db.min()\n",
    "            self.max_val = all_sxx_db.max()\n",
    "        \n",
    "        print(f\"Normalization stats: min={self.min_val:.2f}, max={self.max_val:.2f}\")\n",
    "\n",
    "        # Normalizing & Resizing\n",
    "        print(\"Normalizing and Resizing spectrograms\")\n",
    "        for sxx_db in tqdm(db_spectrograms):\n",
    "            sxx_norm = (sxx_db - self.min_val) / (self.max_val - self.min_val + 1e-40)\n",
    "            self.spectrograms.append(torch.from_numpy(sxx_norm.astype(np.float32)).unsqueeze(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spectrograms[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62badf30-a069-4a73-b9c4-7b751d93e05d",
   "metadata": {},
   "source": [
    "### 2.3 Train / Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e98efd-1ac1-4695-9a66-731c4fb03d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds       = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(dataloader), accuracy, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57e05c-8e34-4d97-a8f8-8b23832409e5",
   "metadata": {},
   "source": [
    "### 2.4 Mean XAI by Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217dccbe-9093-4d1f-981d-8ddbb0403c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ig_mean(model, loader, device, target=0, n_steps=50, internal_batch_size=16, max_batches=None):\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    sum_attr = None\n",
    "    n_used = 0\n",
    "\n",
    "    for batch_idx, (inputs, _) in enumerate(loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        inputs.requires_grad_(True)\n",
    "\n",
    "        baselines = torch.zeros_like(inputs)\n",
    "\n",
    "        attr = ig.attribute(\n",
    "            inputs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            n_steps=n_steps,\n",
    "            internal_batch_size=internal_batch_size,\n",
    "        ) # (B, C, H, W)\n",
    "\n",
    "        batch_sum = attr.sum(dim=0) # (C, H, W)\n",
    "\n",
    "        if sum_attr is None:\n",
    "            sum_attr = batch_sum.detach()\n",
    "        else:\n",
    "            sum_attr += batch_sum.detach()\n",
    "\n",
    "        n_used += inputs.size(0)\n",
    "\n",
    "        del inputs, baselines, attr, batch_sum\n",
    "\n",
    "    mean_attr = sum_attr / n_used\n",
    "    return mean_attr.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec637e9a-c262-48f2-b2ab-b4940d1e8ce4",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32300f0-6413-42e9-a169-ad41f0f9ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI_SAVE_DIR = \"./XAI_During_Training\"\n",
    "os.makedirs(XAI_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "RESNET_VERSIONS = ['resnet34']\n",
    "SAVE_DIR = \"./STFT-ResNet_models_with_XAI/resnet34_stft_hp\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "fs = 2e9\n",
    "LEARNING_RATE = [1e-3, 5e-4, 1e-4]\n",
    "NPERSEG = [64, 128, 256]\n",
    "OVERLAP_RATIO = [0.0, 0.5, 0.75]\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85324739-344a-434d-991b-d31db51f3fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in RESNET_VERSIONS:\n",
    "    for version in AES_VERSIONS:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_name} on {version} with STFT data\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # 1. Load Data\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = load_supervised_set(version)\n",
    "\n",
    "        for nperseg in NPERSEG:\n",
    "            for ratio in OVERLAP_RATIO:\n",
    "                noverlap = int(nperseg * ratio)\n",
    "                \n",
    "                # 2. Min/Max scaling on Train Set\n",
    "                print(\"\\n--- Preparing Train Dataset ---\")\n",
    "                train_dataset = STFTDataset(X_train, y_train, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "                \n",
    "                # Applying Min/Max to Validation/Test Set\n",
    "                print(\"\\n--- Preparing Validation Dataset ---\")\n",
    "                val_dataset   = STFTDataset(X_val, y_val, fit_on_data=train_dataset, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "                print(\"\\n--- Preparing Test Dataset ---\")\n",
    "                test_dataset  = STFTDataset(X_test, y_test, fit_on_data=train_dataset, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "                \n",
    "                # DataLoader\n",
    "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "                test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "                \n",
    "                for lr in LEARNING_RATE:\n",
    "                    # 3. Model / Loss / Optimizer\n",
    "                    model = get_resnet(model_name).to(device)\n",
    "                    criterion = nn.BCEWithLogitsLoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                    \n",
    "                    # 4. Train Loop\n",
    "                    best_val_acc = 0.0\n",
    "                    best_val_loss = float(\"inf\")\n",
    "                    patience_counter = 0\n",
    "                    best_model_path = os.path.join(SAVE_DIR, f\"{version}_{model_name}_stft_n{nperseg}_o{noverlap}_lr{lr}_best.pth\")\n",
    "                    \n",
    "                    ig_epochs = []\n",
    "                    ig_means  = []\n",
    "                    for epoch in tqdm(range(EPOCHS), desc=f\"{version}-{model_name}-n{nperseg}-o{noverlap}-lr{lr}\"):\n",
    "                        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "                        \n",
    "                        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.8f} | Val Loss: {val_loss:.8f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "                        update_model = False\n",
    "                        if val_acc > best_val_acc:\n",
    "                            update_model = True\n",
    "                        elif val_acc == best_val_acc and val_loss < best_val_loss:\n",
    "                            update_model = True\n",
    "        \n",
    "                        if update_model:\n",
    "                            best_val_acc  = val_acc\n",
    "                            best_val_loss = val_loss\n",
    "                            torch.save(model.state_dict(), best_model_path)\n",
    "                            print(f\"[BEST] Epoch {epoch+1} | Val Acc={val_acc:.4f} | Val Loss={val_loss:.8f}\")\n",
    "                            patience_counter = 0\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                        \n",
    "                        mean_attr = compute_ig_mean(model, train_loader, device=device, target=0, n_steps=50, internal_batch_size=BATCH_SIZE, max_batches=None)\n",
    "                        ig_epochs.append(epoch + 1)\n",
    "                        ig_means.append(mean_attr)\n",
    "                        model.train()\n",
    "\n",
    "                        if patience_counter >= PATIENCE:\n",
    "                            print(f\"Early stopping after epoch {epoch+1}.\")\n",
    "                            break\n",
    "        \n",
    "                    ig_save_path = os.path.join(\n",
    "                        XAI_SAVE_DIR,\n",
    "                        f\"{version}_{model_name}_stft_n{nperseg}_o{noverlap}_lr{lr}_IG_HISTORY.pt\"\n",
    "                    )\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"meta\": {\n",
    "                                \"version\": version,\n",
    "                                \"model_name\": model_name,\n",
    "                                \"fs\": fs,\n",
    "                                \"nperseg\": nperseg,\n",
    "                                \"noverlap\": noverlap,\n",
    "                                \"lr\": lr,\n",
    "                                \"n_steps\": 50,\n",
    "                                \"internal_batch_size\": BATCH_SIZE,\n",
    "                            },\n",
    "                            \"epochs\": ig_epochs,\n",
    "                            \"mean\": torch.stack(ig_means, dim=0),  # (E, C, H, W)\n",
    "                        },\n",
    "                        ig_save_path\n",
    "                    )\n",
    "                    \n",
    "                    # 5. Best Model Evaluation\n",
    "                    print(\"\\n--- Best Model Evaluation on Test Set ---\")\n",
    "                    model.load_state_dict(torch.load(best_model_path))\n",
    "                    \n",
    "                    test_loss, test_acc, true_labels, pred_labels = validate(model, test_loader, criterion, device)\n",
    "                    \n",
    "                    metrics, cm = evaluate_model(true_labels, pred_labels)\n",
    "                    print_eval(f\"{version} - {model_name}-n{nperseg}-o{noverlap}-lr{lr} (STFT)\", metrics, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b6429-7fb9-4c83-93ab-55c4a4aa0d09",
   "metadata": {},
   "source": [
    "## 4. Visualize XAI mean by epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24115b-099a-4574-a64f-3912a3d5e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_epoch_xai_progression_history(\n",
    "    xai_dir,\n",
    "    version,\n",
    "    model_name,\n",
    "    nperseg,\n",
    "    noverlap,\n",
    "    lr,\n",
    "    epoch_range=None,\n",
    "    cmap=\"hot\",\n",
    "    use_std=False,\n",
    "    max_cols=10\n",
    "):\n",
    "    fname = f\"{version}_{model_name}_stft_n{nperseg}_o{noverlap}_lr{lr}_IG_HISTORY.pt\"\n",
    "    path = os.path.join(xai_dir, fname)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Not found: {path}\")\n",
    "\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "    epochs = obj[\"epochs\"]                 # list[int]\n",
    "    key = \"std\" if use_std else \"mean\"\n",
    "    arr = obj[key]                         # tensor: (E, C, H, W) or (E, C, L)\n",
    "    # (E, ...) -> numpy\n",
    "    arr = arr.numpy()\n",
    "\n",
    "    # epoch_range filter\n",
    "    if epoch_range is not None:\n",
    "        start, end = epoch_range\n",
    "        keep_idx = [i for i, e in enumerate(epochs) if start <= e <= end]\n",
    "        if len(keep_idx) == 0:\n",
    "            raise RuntimeError(\"No epochs in the given epoch_range.\")\n",
    "        epochs = [epochs[i] for i in keep_idx]\n",
    "        arr = arr[keep_idx]\n",
    "\n",
    "    # squeeze channel dim handle\n",
    "    if arr.ndim >= 3 and arr.shape[1] == 1:\n",
    "        arr = arr[:, 0]\n",
    "\n",
    "    n = len(epochs)\n",
    "\n",
    "    # (L,) -> (1, L)\n",
    "    if arr.ndim == 2:  # (E, L)\n",
    "        arr = arr[:, None, :]\n",
    "\n",
    "    cols = min(n, max_cols)\n",
    "    rows = int(np.ceil(n / cols))\n",
    "\n",
    "    plt.figure(figsize=(4 * cols, 4 * rows))\n",
    "\n",
    "    for i, e in enumerate(epochs):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(arr[i], cmap=cmap, origin=\"lower\")\n",
    "        plt.title(f\"Epoch {e}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    title_metric = \"STD\" if use_std else \"MEAN\"\n",
    "    plt.suptitle(\n",
    "        f\"{version} | {model_name} | n={nperseg}, o={noverlap} | lr={lr} | {title_metric}\",\n",
    "        fontsize=14\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb787b31-92f6-432a-a5d4-9f93af4fe94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in AES_VERSIONS:\n",
    "    for nperseg in NPERSEG:\n",
    "        for ratio in OVERLAP_RATIO:\n",
    "            noverlap = int(nperseg * ratio)\n",
    "            for lr in LEARNING_RATE:\n",
    "                visualize_epoch_xai_progression_history(\n",
    "                    xai_dir=\"./XAI_During_Training\",\n",
    "                    version=version,\n",
    "                    model_name=\"resnet34\",\n",
    "                    nperseg=nperseg,\n",
    "                    noverlap=noverlap,\n",
    "                    lr=lr,\n",
    "                    epoch_range=(1, 30),\n",
    "                    cmap=\"hot\",\n",
    "                    use_std=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b63eaa-f549-4f47-968c-f9974930da9a",
   "metadata": {},
   "source": [
    "## 5. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d7cc9-a2af-4e5b-a026-af9c190729cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in RESNET_VERSIONS:\n",
    "    for version in AES_VERSIONS:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Evaluating {model_name} on {version}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = load_supervised_set(version)\n",
    "        \n",
    "        for nperseg in NPERSEG:\n",
    "            for ratio in OVERLAP_RATIO:\n",
    "                noverlap = int(nperseg * ratio)\n",
    "                train_dataset = STFTDataset(X_train, y_train, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "                test_dataset  = STFTDataset(X_test, y_test, fit_on_data=train_dataset, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "                test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "                \n",
    "                for lr in LEARNING_RATE:\n",
    "\n",
    "                    model_path = f\"{SAVE_DIR}/{version}_{model_name}_stft_n{nperseg}_o{noverlap}_lr{lr}_best.pth\"\n",
    "                    model = get_resnet(model_name).to(device)\n",
    "                    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "                    criterion = nn.BCEWithLogitsLoss()\n",
    "                    test_loss, test_acc, true_labels, pred_labels = validate(model, test_loader, criterion, device)\n",
    "                    metrics, cm = evaluate_model(true_labels, pred_labels)\n",
    "                \n",
    "                    print_eval(f\"{version}-{model_name}-stft_n{nperseg}-o{noverlap}-lr{lr} (STFT)\", metrics, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3462ad-6d41-4e37-93bc-88700942786e",
   "metadata": {},
   "source": [
    "## 6. XAI Mean & Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564caebf-114c-4939-852f-27e8169ee0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_image_features(model, input_tensor, target_class=0):\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "    baselines = torch.zeros_like(input_tensor)\n",
    "    attributions, delta = ig.attribute(input_tensor, baselines, target=target_class, return_convergence_delta=True)\n",
    "    return attributions\n",
    "\n",
    "def compute_mean_xai(model, dataset, indices, target_class):\n",
    "    \"\"\"\n",
    "    특정 클래스 샘플들의 XAI 평균 계산\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    attrs = []\n",
    "\n",
    "    for idx in tqdm(indices):\n",
    "        img, _ = dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        attr = attribute_image_features(model, img, target_class=target_class)\n",
    "        attrs.append(attr.detach().cpu().numpy())\n",
    "\n",
    "    mean_attr = np.mean(np.stack(attrs), axis=0)\n",
    "    return mean_attr.squeeze()  # (H, W)\n",
    "\n",
    "def visualize_mean_xai(\n",
    "    mean_xai,\n",
    "    title=\"Mean XAI (Trojan)\",\n",
    "    cmap=\"coolwarm\",\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    평균 XAI heatmap 시각화\n",
    "    mean_xai: (H, W) numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "\n",
    "    # 대칭 컬러바를 위해 max 절댓값 기준 설정\n",
    "    vmax = np.max(np.abs(mean_xai))\n",
    "\n",
    "    im = plt.imshow(\n",
    "        mean_xai,\n",
    "        cmap=cmap,\n",
    "        vmin=-vmax,\n",
    "        vmax=vmax,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\"\n",
    "    )\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Time Bins\", fontsize=14)\n",
    "    plt.ylabel(\"Frequency Bins\", fontsize=14)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Attribution\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66199eb1-af9c-4309-b346-7b2af84a46fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"resnet34\"\n",
    "XAI_MEAN_SAVE_DIR = f\"{SAVE_DIR}/Mean_XAI\"\n",
    "os.makedirs(XAI_MEAN_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for version in AES_VERSIONS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Computing Mean XAI for {version}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1. Load dataset\n",
    "    # -------------------------------------------------\n",
    "    X_train, y_train, _, _, X_test, y_test = load_supervised_set(version)\n",
    "\n",
    "    for nperseg in NPERSEG:\n",
    "        for ratio in OVERLAP_RATIO:\n",
    "            noverlap = int(nperseg * ratio)\n",
    "            train_dataset = STFTDataset(X_train, y_train, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "            test_dataset  = STFTDataset(X_test, y_test, fit_on_data=train_dataset, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "            for lr in LEARNING_RATE:\n",
    "                # -------------------------------------------------\n",
    "                # 2. Load model\n",
    "                # -------------------------------------------------\n",
    "                model_path = f\"{SAVE_DIR}/{version}_{MODEL_NAME}_stft_n{nperseg}_o{noverlap}_lr{lr}_best.pth\"\n",
    "                model = get_resnet(MODEL_NAME).to(device)\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "                model.eval()\n",
    "            \n",
    "                # -------------------------------------------------\n",
    "                # 3. Trojan indices (고정 가정)\n",
    "                # -------------------------------------------------\n",
    "                normal_indices = list(range(0, 1000))\n",
    "                trojan_indices = list(range(1000, 2000))\n",
    "            \n",
    "                # -------------------------------------------------\n",
    "                # 4. Compute Mean XAI (Trojan only)\n",
    "                # -------------------------------------------------\n",
    "                mean_normal_xai = compute_mean_xai(\n",
    "                    model=model,\n",
    "                    dataset=test_dataset,\n",
    "                    indices=normal_indices,\n",
    "                    target_class=0\n",
    "                )\n",
    "            \n",
    "                mean_trojan_xai = compute_mean_xai(\n",
    "                    model=model,\n",
    "                    dataset=test_dataset,\n",
    "                    indices=trojan_indices,\n",
    "                    target_class=0\n",
    "                )\n",
    "                \n",
    "                mean_diff_xai = np.abs(mean_trojan_xai - mean_normal_xai)\n",
    "            \n",
    "                # -------------------------------------------------\n",
    "                # 5. Visualize & Save\n",
    "                # -------------------------------------------------\n",
    "                visualize_mean_xai(\n",
    "                    mean_normal_xai,\n",
    "                    title=f\"{version} | Mean XAI (Normal) \\n (nperseg={nperseg}, noverlap={noverlap}, lr={lr})\",\n",
    "                    save_path=f\"{XAI_MEAN_SAVE_DIR}/{version}_{MODEL_NAME}_stft_n{nperseg}_o{noverlap}_lr{lr}_MeanXAI_Normal.png\"\n",
    "                )\n",
    "            \n",
    "                visualize_mean_xai(\n",
    "                    mean_trojan_xai,\n",
    "                    title=f\"{version} | Mean XAI (Trojan) \\n (nperseg={nperseg}, noverlap={noverlap}, lr={lr})\",\n",
    "                    save_path=f\"{XAI_MEAN_SAVE_DIR}/{version}_{MODEL_NAME}_stft_n{nperseg}_o{noverlap}_lr{lr}_MeanXAI_Trojan.png\"\n",
    "                )\n",
    "            \n",
    "                visualize_mean_xai(\n",
    "                    mean_diff_xai,\n",
    "                    title=f\"{version} | Mean XAI Difference (Trojan − Normal) \\n (nperseg={nperseg}, noverlap={noverlap}, lr={lr})\",\n",
    "                    save_path=f\"{XAI_MEAN_SAVE_DIR}/{version}_{MODEL_NAME}_stft_n{nperseg}_o{noverlap}_lr{lr}_MeanXAI_Diff.png\"\n",
    "                )\n",
    "            \n",
    "                print(f\"[DONE] Saved mean XAI to {XAI_MEAN_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f93416-2bd2-4627-a791-7a1f7647052e",
   "metadata": {},
   "source": [
    "## 7. Visualize STFT Mean & Diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf5258-a3f2-45e1-848a-2db1c4d9623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_normalize_spectrograms(X_data, fs=2e9, nperseg=128, noverlap=96):\n",
    "    db_spectrograms = []\n",
    "    print(\"Generating spectrograms...\")\n",
    "    for waveform in tqdm(X_data):\n",
    "        _, _, Zxx = stft(waveform,fs=fs, nperseg=nperseg, noverlap=noverlap, boundary=None, padded=False)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        Sxx_db = 20 * np.log10(Sxx + 1e-9)\n",
    "        db_spectrograms.append(Sxx_db)\n",
    "    \n",
    "    print(\"Calculating global min/max for normalization...\")\n",
    "    all_sxx_db = np.stack(db_spectrograms)\n",
    "    min_val = all_sxx_db.min()\n",
    "    max_val = all_sxx_db.max()\n",
    "    print(f\"Normalization stats: min={min_val:.2f}, max={max_val:.2f}\")\n",
    "\n",
    "    final_spectrograms = []\n",
    "    print(\"Normalizing and resizing spectrograms...\")\n",
    "    for sxx_db in tqdm(db_spectrograms):\n",
    "        sxx_norm = (sxx_db - min_val) / (max_val - min_val + 1e-40)\n",
    "        final_spectrograms.append(sxx_norm.astype(np.float32))\n",
    "        \n",
    "    return np.stack(final_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e19b36-efe6-4916-9f91-71a22d5bb9c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in [\"AES-T600\", \"AES-T1600\"]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing Mean Spectrogram Difference for: {version}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    X_train, y_train, _, _, _, _ = load_supervised_set(version)\n",
    "    for nperseg in NPERSEG:\n",
    "        for ratio in OVERLAP_RATIO:\n",
    "            noverlap = int(nperseg * ratio)\n",
    "            \n",
    "            processed_spectrograms = generate_and_normalize_spectrograms(X_train, fs, nperseg, noverlap)\n",
    "            \n",
    "            # 3. Normal/Triggered Spectrogram\n",
    "            normal_spectrograms = processed_spectrograms[y_train == 0]\n",
    "            triggered_spectrograms = processed_spectrograms[y_train == 1]\n",
    "            \n",
    "            print(f\"\\nNormal shape={normal_spectrograms.shape}, Triggered shape={triggered_spectrograms.shape}\")\n",
    "        \n",
    "            # 4. Mean Spectrogram by class\n",
    "            mean_normal_spec = np.mean(normal_spectrograms, axis=0)\n",
    "            mean_triggered_spec = np.mean(triggered_spectrograms, axis=0)\n",
    "        \n",
    "            # 5. Diff Mean\n",
    "            difference_map = np.abs(mean_triggered_spec - mean_normal_spec)\n",
    "            \n",
    "            # 6. Visualization\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "            fig.suptitle(f\"Mean Spectrogram Analysis for {version}\", fontsize=20)\n",
    "            \n",
    "            TITLE_FONTSIZE = 20\n",
    "            LABEL_FONTSIZE = 18\n",
    "            TICK_FONTSIZE = 16\n",
    "            \n",
    "            # Plot 1: Mean Normal Spectrogram\n",
    "            im1 = axes[0].imshow(mean_normal_spec, cmap='viridis', aspect='auto', origin='lower')\n",
    "            axes[0].set_title('Mean Spectrogram (Disabled)', fontsize=TITLE_FONTSIZE)\n",
    "            axes[0].set_xlabel('Time Bins', fontsize=LABEL_FONTSIZE)\n",
    "            axes[0].set_ylabel('Frequency Bins', fontsize=LABEL_FONTSIZE)\n",
    "            axes[0].tick_params(axis='both', which='major', labelsize=TICK_FONTSIZE)\n",
    "            cb1 = fig.colorbar(im1, ax=axes[0])\n",
    "            cb1.ax.tick_params(labelsize=TICK_FONTSIZE)\n",
    "            \n",
    "            # Plot 2: Mean Triggered Spectrogram\n",
    "            im2 = axes[1].imshow(mean_triggered_spec, cmap='viridis', aspect='auto', origin='lower')\n",
    "            axes[1].set_title('Mean Spectrogram (Triggered)', fontsize=TITLE_FONTSIZE)\n",
    "            axes[1].set_xlabel('Time Bins', fontsize=LABEL_FONTSIZE)\n",
    "            axes[1].tick_params(axis='both', which='major', labelsize=TICK_FONTSIZE)\n",
    "            cb2 = fig.colorbar(im2, ax=axes[1])\n",
    "            cb2.ax.tick_params(labelsize=TICK_FONTSIZE)\n",
    "            \n",
    "            # Plot 3: Diff Map\n",
    "            max_abs_diff = np.abs(difference_map).max()\n",
    "            im3 = axes[2].imshow(difference_map, cmap='coolwarm', vmin=-max_abs_diff, vmax=max_abs_diff, aspect='auto', origin='lower')\n",
    "            axes[2].set_title('Difference |Triggered - Disabled|', fontsize=TITLE_FONTSIZE)\n",
    "            axes[2].set_xlabel('Time Bins', fontsize=LABEL_FONTSIZE)\n",
    "            axes[2].tick_params(axis='both', which='major', labelsize=TICK_FONTSIZE)\n",
    "            cb3 = fig.colorbar(im3, ax=axes[2])\n",
    "            cb3.set_label('Magnitude Difference', size=LABEL_FONTSIZE)\n",
    "            cb3.ax.tick_params(labelsize=TICK_FONTSIZE)\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
